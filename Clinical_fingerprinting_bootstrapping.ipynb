{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "This file is part of the project megFingerprinting. All of megFingerprinting code is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. megFingerprinting is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with megFingerprinting. If not, see https://www.gnu.org/licenses/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from fuzzywuzzy import fuzz\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import scipy.io as sio\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "sns.set(font_scale=2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(sns.color_palette(\"husl\", 8))\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), sp.stats.sem(a)\n",
    "    h = np.percentile(a, (1-((1-confidence)/2))*100)\n",
    "    l = np.percentile(a, ((1-confidence)/2)*100)\n",
    "    return m, l, h\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap values of differentiation accuracy Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9013000000000002, 0.88, 0.94)\n",
      "(2.9877339930181672, 0.9729036783198106, 4.4771913149606375)\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "# Parameters\n",
    "n_subs = 54 # Change here to get number of participants! \n",
    "n_feats = int(68*451)\n",
    "n_measurements = 2\n",
    "\n",
    "# Warangle data set into two big feature matrices\n",
    "def prune_subject_csv(filename):\n",
    "    '''\n",
    "    This function takes in the subject's csv file from MATLAB, takes out the \n",
    "    doubled correlations (because of symmetry) and outputs a numpy array ready to be concatenated\n",
    "    in the grand feature matrix\n",
    "    Args:\n",
    "        filename (string): Name of the csv matrix\n",
    "    Returns: \n",
    "        sub_feat (np.array): Subject's features \n",
    "    '''\n",
    "    #print(filename[19:23])\n",
    "    sub_feat = np.zeros([1, (n_feats)+1]) # Number of unique values in corr matrix + subject label\n",
    "    psd_matrix = pd.read_csv(filename, header=None)\n",
    "    mat=np.asmatrix(psd_matrix)\n",
    "    sub_feat[0, :-1]=mat[:,0:451].flatten()\n",
    "    sub_feat[0, -1] = int(filename[19:23])    \n",
    "    return sub_feat\n",
    "\n",
    "\n",
    "# Get n subjects: both training and testing datasets\n",
    "onlyfiles = [f for f in listdir('NEWspectraCTL/') if isfile(join('NEWspectraCTL/', f))]\n",
    "sub_target = np.zeros([n_subs, (n_feats)+1])\n",
    "sub_database = np.zeros([n_subs, (n_feats)+1])\n",
    "iv = 0\n",
    "it = 0\n",
    "for iFile in sorted(onlyfiles)[0:(n_subs*2)]: \n",
    "    sub = 'NEWspectraCTL/' + iFile\n",
    "    #print(sub)\n",
    "    #print(sub[33])\n",
    "    if sub[28] == 'v':\n",
    "        sub_target[iv, :] = prune_subject_csv(sub)\n",
    "        iv += 1\n",
    "    else:\n",
    "        sub_database[it, :] = prune_subject_csv(sub)\n",
    "        it += 1\n",
    "        \n",
    "        \n",
    "n_subs=50\n",
    "niter=1000\n",
    "bootstrap = np.zeros([niter, 2])\n",
    "self_id =np.zeros([n_subs, niter])\n",
    "\n",
    "for j in range(0,niter):\n",
    "    index=sample(range(54), n_subs)\n",
    "    Tempsub_target=sub_target[index,:]\n",
    "    Tempsub_database=sub_database[index,:]\n",
    "    # Correlations can be computed as the dot product between two z-scored vectors\n",
    "    z_target = sp.stats.zscore(Tempsub_target[:, :-1], axis = 1)\n",
    "    z_database = sp.stats.zscore(Tempsub_database[:,:-1], axis = 1)\n",
    "    predictions = z_target.dot(z_database.transpose()) / (Tempsub_database.shape[1] - 1) # target, database\n",
    "    bootstrap[j,0] = accuracy_score(range(n_subs), predictions.argmax(axis = 1))\n",
    "    bootstrap[j,1] = accuracy_score(range(n_subs), predictions.argmax(axis = 0))\n",
    "    self_id[0:n_subs,j] = np.diagonal(sp.stats.zscore(predictions, axis = 1))\n",
    "\n",
    "print(mean_confidence_interval(bootstrap.flatten()))\n",
    "\n",
    "print(mean_confidence_interval(self_id.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7806500000000002, 0.7428571428571429, 0.8285714285714286)\n",
      "(2.621978089768564, 0.31333489490631034, 4.4058416861660925)\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "# Parameters\n",
    "n_subs = 79 # Change here to get number of participants! \n",
    "n_feats = int(68*451)\n",
    "n_measurements = 2\n",
    "\n",
    "# Warangle data set into two big feature matrices\n",
    "def prune_subject_csv(filename):\n",
    "    '''\n",
    "    This function takes in the subject's csv file from MATLAB, takes out the \n",
    "    doubled correlations (because of symmetry) and outputs a numpy array ready to be concatenated\n",
    "    in the grand feature matrix\n",
    "    Args:\n",
    "        filename (string): Name of the csv matrix\n",
    "    Returns: \n",
    "        sub_feat (np.array): Subject's features \n",
    "    '''\n",
    "    #print(filename[19:23])\n",
    "    sub_feat = np.zeros([1, (n_feats)+1]) # Number of unique values in corr matrix + subject label\n",
    "    psd_matrix = pd.read_csv(filename, header=None)\n",
    "    mat=np.asmatrix(psd_matrix)\n",
    "    sub_feat[0, :-1]=mat[:,0:451].flatten()\n",
    "    sub_feat[0, -1] = int(filename[19:23])    \n",
    "    return sub_feat\n",
    "\n",
    "\n",
    "# Get n subjects: both training and testing datasets\n",
    "onlyfiles = [f for f in listdir('NEWspectraPKD/') if isfile(join('NEWspectraPKD/', f))]\n",
    "sub_target = np.zeros([n_subs, (n_feats)+1])\n",
    "sub_database = np.zeros([n_subs, (n_feats)+1])\n",
    "iv = 0\n",
    "it = 0\n",
    "for iFile in sorted(onlyfiles)[0:(n_subs*2)]: \n",
    "    sub = 'NEWspectraPKD/' + iFile\n",
    "    #print(sub)\n",
    "    #print(sub[33])\n",
    "    if sub[28] == 'v':\n",
    "        sub_target[iv, :] = prune_subject_csv(sub)\n",
    "        iv += 1\n",
    "    else:\n",
    "        sub_database[it, :] = prune_subject_csv(sub)\n",
    "        it += 1\n",
    "        \n",
    "        \n",
    "n_subs=70\n",
    "niter=1000\n",
    "bootstrap = np.zeros([niter, 2])\n",
    "self_id =np.zeros([n_subs, niter])\n",
    "\n",
    "for j in range(0,niter):\n",
    "    index=sample(range(79), n_subs)\n",
    "    Tempsub_target=sub_target[index,:]\n",
    "    Tempsub_database=sub_database[index,:]\n",
    "    # Correlations can be computed as the dot product between two z-scored vectors\n",
    "    z_target = sp.stats.zscore(Tempsub_target[:, :-1], axis = 1)\n",
    "    z_database = sp.stats.zscore(Tempsub_database[:,:-1], axis = 1)\n",
    "    predictions = z_target.dot(z_database.transpose()) / (Tempsub_database.shape[1] - 1) # target, database\n",
    "    bootstrap[j,0] = accuracy_score(range(n_subs), predictions.argmax(axis = 1))\n",
    "    bootstrap[j,1] = accuracy_score(range(n_subs), predictions.argmax(axis = 0))\n",
    "    self_id[0:n_subs,j] = np.diagonal(sp.stats.zscore(predictions, axis = 1))\n",
    "\n",
    "print(mean_confidence_interval(bootstrap.flatten()))\n",
    "\n",
    "print(mean_confidence_interval(self_id.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8146772151898738, 0.810126582278481, 0.8354430379746836)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "from random import sample\n",
    "n_subs = 133 # Change here to get number of participants! \n",
    "n_feats = int(68*451)\n",
    "n_measurements = 2\n",
    "\n",
    "# Warangle data set into two big feature matrices\n",
    "def prune_subject_csv(filename):\n",
    "    '''\n",
    "    This function takes in the subject's csv file from MATLAB, takes out the \n",
    "    doubled correlations (because of symmetry) and outputs a numpy array ready to be concatenated\n",
    "    in the grand feature matrix\n",
    "    Args:\n",
    "        filename (string): Name of the csv matrix\n",
    "    Returns: \n",
    "        sub_feat (np.array): Subject's features \n",
    "    '''\n",
    "    #print(filename)\n",
    "    #print(filename[19:23])\n",
    "    sub_feat = np.zeros([1, (n_feats)+1]) # Number of unique values in corr matrix + subject label\n",
    "    psd_matrix = pd.read_csv(filename, header=None)\n",
    "    mat=np.asmatrix(psd_matrix)\n",
    "    sub_feat[0, :-1]=mat[:,0:451].flatten()\n",
    "    sub_feat[0, -1] = int(filename[19:23])    \n",
    "    return sub_feat\n",
    "\n",
    "\n",
    "# Get n subjects: both training and testing datasets\n",
    "onlyfiles = [f for f in listdir('NEWspectraFUL/') if isfile(join('NEWspectraFUL/', f))]\n",
    "sub_target = np.zeros([n_subs, (n_feats)+1])\n",
    "sub_database = np.zeros([n_subs, (n_feats)+1])\n",
    "iv = 0\n",
    "it = 0\n",
    "for iFile in sorted(onlyfiles)[0:(n_subs*2)]: \n",
    "    sub = 'NEWspectraFUL/' + iFile\n",
    "    #print(sub)\n",
    "    #print(sub[28])\n",
    "    if sub[28] == 'v':\n",
    "        sub_target[iv, :] = prune_subject_csv(sub)\n",
    "        iv += 1\n",
    "    else:\n",
    "        sub_database[it, :] = prune_subject_csv(sub)\n",
    "        it += 1\n",
    "        \n",
    "        \n",
    "n_subs=133\n",
    "n_subs_2=51\n",
    "niter=1000\n",
    "acc_over=np.zeros([niter,2])\n",
    "\n",
    "for j in range(0,niter):\n",
    "    self_acc=np.zeros([133,2])\n",
    "    for i in range(54,133):\n",
    "        np_arr1=np.array(i)\n",
    "        np_arr2= sample(range(0,54), 50)\n",
    "        index=np.append(np_arr2, np_arr1)\n",
    "\n",
    "        sub_target_1=sub_target[index, :]\n",
    "        sub_database_1=sub_database[index, :]\n",
    "\n",
    "        # Correlations can be computed as the dot product between two z-scored vectors\n",
    "        z_target = sp.stats.zscore(sub_target_1[:, :-1], axis = 1)\n",
    "        z_database = sp.stats.zscore(sub_database_1[:,:-1], axis = 1)\n",
    "        predictions = z_target.dot(z_database.transpose()) / (sub_database.shape[1] - 1) # target, database\n",
    "        target_from_database = accuracy_score(range(n_subs_2), predictions.argmax(axis = 1))\n",
    "        database_from_target = accuracy_score(range(n_subs_2), predictions.argmax(axis = 0))\n",
    "        \n",
    "        self_id = np.diagonal(sp.stats.zscore(predictions, axis = 1))\n",
    "        cor_predictions=np.diagonal(predictions)     \n",
    "        self_acc[i,0]= (predictions.argmax(axis = 1)[-1]==50)\n",
    "        self_acc[i,1]= (predictions.argmax(axis = 0)[-1]==50)\n",
    "        #print(self_acc)\n",
    "    acc_over[j,0]=  (sum(self_acc)/79)[0]\n",
    "    acc_over[j,1]=  (sum(self_acc)/79)[1]\n",
    "            \n",
    "print(mean_confidence_interval(acc_over.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
